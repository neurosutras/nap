I) To install on a local machine:

1) install anaconda python, includes ipython, numpy, matplotlib

2) download neuron from:
http://www.neuron.yale.edu/ftp/neuron/versions/alpha/nrn-7.4.rel-1390.tar.gz

or install from mercurial
cd ~/neuron
hg clone http://www.neuron.yale.edu/hg/neuron/nrn -r trunk
hg clone http://www.neuron.yale.edu/hg/neuron/iv

following tips from:

http://www.neuron.yale.edu/neuron/download/compilestd_osx

http://www.neuron.yale.edu/phpBB/viewtopic.php?f=4&t=3051#p12584

and making sure to execute these commands before running 'make':

export CFLAGS='-Qunused-arguments'
export CXXFLAGS='-Qunused-arguments'

cd nrn/src/nrnmpi
sh mkdynam.sh

3) Install btmorph from

http://btmorph.readthedocs.org/en/latest/readme.html#installation

4) Make sure ~/neuron/nrnenv includes:

export IDIR=/Applications/NEURON-7.4
export IV=$IDIR/iv
export N=$IDIR/nrn
export CPU=x86_64
export PATH=$IV/$CPU/bin:$N/$CPU/bin:$PATH


and ~/.bash_profile includes:

export HOME=/Users/milsteina
export PATH="$HOME/anaconda/bin:$PATH"
export PATH=$HOME/local/bin:$PATH
source $HOME/neuron/nrnenv
export PATH="/Applications/NEURON-7.4:$PATH"
export PYTHONPATH="/Applications/NEURON-7.4/nrn/lib/python:$PYTHONPATH"
export PYTHONPATH="$HOME/python_modules/btmorph:$PYTHONPATH"
export PATH=$PYTHONPATH:$PATH

5) If error related to libreadline.6.2.dylib :

cd /Applications/NEURON-7.4/nrn/lib/python/neuron/
install_name_tool -change libreadline.6.2.dylib $HOME/anaconda2/lib/libreadline.6.2.dylib hoc.so

II) To install on a linux cluster:

1) Download the latest tar.gz from http://www.neuron.yale.edu/ftp/neuron/versions/
e.g.
wget http://www.neuron.yale.edu/ftp/neuron/versions/alpha/nrn-7.4.rel-1324.tar.gz

2) Unpack the file:
tar xvzf nrn-7.4.rel-1324.tar.gz

3) create these subdirectories in your home directory on the cluster: /neuron/nrn

4) install following these directions:
http://www.neuron.yale.edu/neuron/download/compile_linux

from the directory containing the unzipped install files:

*
if cloning from mercurial:
./build.sh
*

On Comet:
module load python
module load hdf5
module load mpi4py

./configure --prefix=$HOME/neuron/nrn --without-iv --with-paranrn --with-nrnpython --with-mpi


On Cori:
module swap PrgEnv-intel PrgEnv-gnu
module load python
export CC=cc
export CXX=CC
export LD_PRELOAD=/lib64/libreadline.so.6
export CRAYPE_LINK_TYPE=dynamic
export PYTHONPATH=/usr/common/software/python/2.7-anaconda-4.4/lib/python2.7/site-packages:$PYTHONPATH

./configure --prefix=$HOME/neuron/nrn -with-paranrn --with-mpi --with-nrnpython=/usr/common/software/python/2.7-anaconda-4.4/bin/python --without-x --without-memacs --with-readline=no
make
make install

- produces some long pauses with error messages related to a deprecated NumPy API, I think this is a strange interaction with anaconda python2.7, but it doesn't appear to cause any problems.

5) Make sure ~/neuron/nrnenv includes:

export IDIR=$HOME/neuron
export N=$IDIR/nrn
export CPU=x86_64
export PATH=$N/$CPU/bin:$PATH

6) Make sure ~/.bash_profile includes:

export PATH=/usr/local/anaconda/bin:$PATH
source $HOME/neuron/nrnenv
export PATH=$HOME/neuron/nrn:$PATH
export PYTHONPATH=$HOME/neuron/nrn/lib/python:$PYTHONPATH
export PYTHONPATH=$HOME/python_modules/btmorph:$PYTHONPATH
export PYTHONPATH=$HOME/CA1Sim:$PYTHONPATH

7) mpi4py is required to use the IPython.parallel framework across cores on more than one node, and it is already installed alongside anaconda on the Janelia cluster.
In order to use the Intel MPI implementation on the Janelia cluster, make sure the following lines are included in ~/.bashrc

if [ -f /usr/local/INTEL2016.sh ]; then
  . /usr/local/INTEL2016.sh
fi

export I_MPI_DEBUG=0
export I_MPI_RENDEZVOUS_RDMA_WRITE=1
export I_MPI_DEVICE=rdssm:OpenIB-iwarp
export I_MPI_FALLBACK_DEVICE=0
export I_MPI_USE_DYNAMIC_CONNECTIONS=0

8) In order for the IPython use more than one node on the cluster

A custom ipython profile must be used for the IPython.parallel framework to work with an MPI backend on the Janelia cluster.

ipython profile create --parallel --profile=mpi
edit the file IPYTHONDIR/profile_mpi/ipcluster_config.py
c.IPClusterEngines.engine_launcher_class = 'MPIEngineSetLauncher'
c.IPClusterEngines.work_dir = u'/groups/magee/home/milsteina/CA1Sim/'

edit the file IPYTHONDIR/profile_mpi/ipengine_config.py
c.MPI.use = 'mpi4py'

9) set the default backend for matplotlib to 'Agg' in the matplotlibrc file so that importing pyplot doesn't cause errors in a system without display capability
Original config file should be in site-packages/matplotlib/mpl-data/matplotlibrc
Copy it to $HOME/.config/matplotlib/matplotlibrc, then change the backend to 'Agg'.

On my mac, I have to manually edit matplotlibrc in site-packages/matplotlib/mpl-data to
'Qt5Agg' every time I update anaconda.


10) To run my simulations, make sure to execute nrnivmodl in the directory that contains the .mod and .py files.
Make sure to put .swc files in the /morphologies directory and expect to find .pkl and .hdf5 data output files in the
/data directory

Then start an iPython session with: ipython

and execute pieces of the similation with:
run name_of_py_file

build your own cell with

from specify_cells import *
from function_lib import *
from plot_results import *

cell = HocCell()
cell.make_section('soma')

You can also import a morphology from an .swc file:

cell = HocCell('str_with_name_of_swc_file.swc')

BtMorph requires that the compartments with indices 1, 2, and 3 all be of type 1 (soma), according to the standard used by NeuroMorpho.org . My model expects compartment types to be labeled as follows (1 = soma, 2 = axon, 3 = basal, 4 = apical, 5 = trunk, 6 = tuft). Soma and axon compartments are discarded and replaced with a simplified representation of soma and axon.

Ion channel mechanisms and cable parameters for various types of compartments can be inserted with commands like:

cell.modify_mech_param('soma', 'cable', 'Ra', 150.)
cell.modify_mech_param('soma', 'pas', 'g', 0.0002)
cell.modify_mech_param('basal', 'h', 'ghbar', origin='soma')
cell.modify_mech_param(sec_type, 'kap', 'gkabar', origin='soma', slope=3.84e-4, max=0.24)

Once you have a cell appropriately specified, you can save a file containing the mechanisms with:

cell.export_mech_dict('str_with_name_of_pkl_file.pkl'), or with no arg to automatically generate a unique filename with
the data and timestamp.

Then a cell can be instantiated with a mech_dict file:

cell = HocCell('str_with_name_of_swc_file.swc', 'str_with_name_of_pkl_file.pkl')

A quick simulation using adaptive timestep integration can be run with:

sim = QuickSim(duration)
sim.append_rec(cell, cell.tree.root, loc=0.5, description='soma')
sim.run()
sim.plot()

or

sim.export_to_file(name_of_hdf5_object, index_of_similation_in_file)

then calling a plot function on the output file:

plot_superimpose_conditions('str_with_name_of_output_file') # no suffix necessary


11) PyCharm limits the size of the console output buffer. Change the value of idea.cycle.buffer.size in the idea.properties file in the /bin directory of the install package. To change the size of the terminal output buffer, change the registry key terminal.buffer.max.lines.count. Navigate to Help| Find action| Type "Registry"| Find terminal.buffer.max.lines.count.

12) Building h5py against a parallelized, MPI-enabled version of HDF5 is a giant headache. The anaconda packages do not allow this option. pip and homebrew can sort of be used for some pieces, but one has to be VERY careful to not have conflicting installs, and the PATH and LD_LIBRARY_PATH must be set correctly in .bash_profile

- install mpich via homebrew:
brew install mpich
add this to $PATH:
/usr/local/Cellar/mpich/3.2_2/bin
add this to $LD_LIBRARY_PATH:
/usr/local/Cellar/mpich/3.2_2/lib

build parallel hdf5 from source, making sure that which mpicc refers to the above mpich installation directory. follow instructions from:
https://support.hdfgroup.org/ftp/HDF5/current/src/unpacked/release_docs/INSTALL_parallel
and
http://docs.h5py.org/en/latest/mpi.html

sudo mkdir /usr/local/hdf5
CC=mpicc ./configure --enable-parallel --enable-shared --prefix=/usr/local/hdf5
make
make check
sudo make install
add this to $PATH:
/usr/local/hdf5/bin
add this to $LD_LIBRARY_PATH:
/usr/local/hdf5/lib

as long as the proper mpicc is in the search path, pip can work for mpi4py:
http://pythonhosted.org/mpi4py/usrman/install.html#using-pip-or-easy-install
pip install mpi4py

test it with:
# test_mpi.py
from mpi4py import MPI
rank = MPI.COMM_WORLD.rank  # The process ID (integer 0-3 for 4-process run)
print "Hello World (from process %d)" % rank
#
run with:
mpiexec -n 4 python test_mpi.py

finally, build h5py to refer to parallel hdf5 and mpi4py:
http://docs.h5py.org/en/latest/mpi.html
download source from:
https://pypi.python.org/pypi/h5py

export CC=mpicc
python setup.py configure --mpi --hdf5=/usr/local/hdf5
sudo python setup.py build --build-base=/Users/milsteina/anaconda2/
sudo python setup.py install --prefix=/Users/milsteina/anaconda2/

test it with:
# test_phdf5.py
from mpi4py import MPI
import h5py
rank = MPI.COMM_WORLD.rank  # The process ID (integer 0-3 for 4-process run)
print "Hello World (from process %d)" % rank
f = h5py.File('parallel_test.hdf5', 'w', driver='mpio', comm=MPI.COMM_WORLD)
dset = f.create_dataset('test', (4,), dtype='i')
dset[rank] = rank
f.close()
#
run with:
mpiexec -n 4 python test_phdf5.py

can use serial h5py to view the contents of the file in an ipython session.